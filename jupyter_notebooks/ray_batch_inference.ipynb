{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40881321-54b2-44d2-8716-77d5a7f591e3",
   "metadata": {},
   "source": [
    "## Importing Required Libraries\n",
    "\n",
    "This cell imports the key libraries needed for distributed deep learning and image processing:\n",
    "\n",
    "- `ray`: Enables scalable distributed computing for model training and data processing.\n",
    "- `torch`: Core PyTorch package for building and training neural networks.\n",
    "- `torchvision.models` and `torchvision.transforms`: Provides access to pretrained models (like ResNet152) and common image transformations.\n",
    "- `PIL.Image`: Supports image loading and manipulation using the Pillow library.\n",
    "- `numpy`: Essential package for efficient numerical operations and array manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb728dd-2f9e-4be6-ad72-27ce5ff55875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray \n",
    "import torch \n",
    "from torchvision.models import resnet152, ResNet152_Weights\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86471aa-928d-45be-ab72-30bde4ce1c33",
   "metadata": {},
   "source": [
    "### Loading a Sample Image Batch from S3 with Ray Data\n",
    "\n",
    "This cell demonstrates how to efficiently load image data from an S3 bucket using Ray Data:\n",
    "\n",
    "- Reads images from an S3 URI in RGB mode into a Ray Dataset.\n",
    "- Subsets the dataset to the first 1,000 images for quick experimentation.\n",
    "- Retrieves a batch of 3 images as a dictionary of arrays.\n",
    "- Converts the first image in the batch from a NumPy array to a PIL Image for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4055486-7a57-430b-b43d-944d8320a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri = \"s3://anonymous@air-example-data-2/imagenette2/train/\"\n",
    "ds = ray.data.read_images(s3_uri, mode=\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa43265-8e42-4e03-9c45-e92b0f8a87f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_ds = ds.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edb37b0-1226-44f8-a436-c1d01c1cdeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch = subset_ds.take_batch(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211a33d-8a70-472b-8b42-a0986d4ec0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.fromarray(single_batch[\"image\"][0])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de290199-c4c2-4658-837e-8d5e224188bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc3986-5ced-4a8c-82cc-7ec028b96f25",
   "metadata": {},
   "source": [
    "### Applying Preprocessing Transforms to Images from Ray Dataset\n",
    "\n",
    "This section defines and applies preprocessing steps to each image in the Ray Dataset:\n",
    "\n",
    "- Loads the standard ImageNet normalisation and resizing transforms for ResNet-152.\n",
    "- Combines the transforms with `ToTensor()` using `transforms.Compose`.\n",
    "- Defines a `preprocess_image` function to:\n",
    "  - Store the original image.\n",
    "  - Apply the full transform pipeline, saving the result as `\"transformed_image\"`.\n",
    "- Applies this function to every row in the dataset with `.map()`, creating a new dataset with both original and transformed images.\n",
    "- Retrieves two samples as a batch and displays the available keys and the shape of the transformed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a99804-14dc-4e1d-8364-1fe0315575c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet152_Weights.IMAGENET1K_V1\n",
    "imagenet_transforms = weights.transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(), imagenet_transforms()])\n",
    "\n",
    "def preprocess_image(row: dict[str, np.ndarray]):\n",
    "    return {\n",
    "        \"original_image\": row[\"image\"],\n",
    "        \"transformed_image\": transform(row[\"image\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96694478-8d1b-4ae5-9969-03de1f2d1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_ds = ds.map(preprocess_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8933c-ee00-4fe8-a608-271d96be2fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_batches = transformed_ds.take_batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fec2a-30b6-45e6-ac49-5677a944d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Batch is a dictionary with the following keys : {two_batches.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415d42f1-d6ed-4ee1-9c41-a3ca4a971af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_batches[\"transformed_image\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61331ca9-ae63-4a6f-a05b-82f05bc8a784",
   "metadata": {},
   "source": [
    "### Batch Inference Class for Distributed Prediction with ResNet-152\n",
    "\n",
    "This class encapsulates batch inference logic for ResNet-152 using PyTorch and Ray Data:\n",
    "\n",
    "- **Initialisation**:  \n",
    "  - Loads ResNet-152 with pretrained ImageNet weights.\n",
    "  - Sets the model to evaluation mode for inference.\n",
    "- **Call Method**:  \n",
    "  - Accepts a batch of preprocessed images (as NumPy arrays).\n",
    "  - Converts the batch to a PyTorch tensor and moves it to the appropriate device.\n",
    "  - Performs inference in no-grad mode for efficiency.\n",
    "  - Returns both the predicted labels and the original images.\n",
    "\n",
    "> **Note:**  \n",
    "> When used with Ray Data’s `.map_batches()` or `.map()`, an instance of this class is automatically created on each Ray worker (CPU or GPU).  \n",
    "> Ray Data manages distributed execution and device placement—you do **not** need to use `@ray.remote` on this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401cfbe0-a92e-4e34-a81f-680a1edb67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchInferenceResNet:\n",
    "    def __init__(self):\n",
    "        self.weights = ResNet152_Weights.IMAGENET1K_V1\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = resnet152(weights=self.weights).to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def __call__(self, batch: dict[str, np.ndarray]):\n",
    "        torch_batch = torch.from_numpy(batch[\"transformed_image\"]).to(self.device)\n",
    "        with torch.inference_mode():\n",
    "            prediction = self.model(torch_batch)\n",
    "            predicted_classes = prediction.argmax(dim=1).detach().cpu()\n",
    "            predicted_labels = [\n",
    "                self.weights.meta[\"categories\"][i] for i in predicted_classes\n",
    "            ]\n",
    "            return {\n",
    "                \"predicted_label\": predicted_labels,\n",
    "                \"original_image\": batch[\"original_image\"],\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068eed24-2abd-4c63-93e2-a75fdaf55060",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = transformed_ds.map_batches(\n",
    "    BatchInferenceResNet,\n",
    "    concurrency=4,  \n",
    "    #num_gpus=1,  \n",
    "    batch_size=10,\n",
    ")\n",
    "prediction_batch = predictions.take_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b73e2-2a18-48e0-b579-acf23de28fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, prediction in zip(\n",
    "    prediction_batch[\"original_image\"], prediction_batch[\"predicted_label\"]\n",
    "):\n",
    "    img = Image.fromarray(image)\n",
    "    display(img)\n",
    "    print(\"Label: \", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7456c33-f7c2-4f86-a6c3-d274efa3b02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483dc4bd-c44c-4642-8003-b987a687c0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
