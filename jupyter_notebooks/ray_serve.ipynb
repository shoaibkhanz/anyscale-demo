{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9a6cc8-72b9-44a4-96cf-07eb15612541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ray serve and FastAPI libraries\n",
    "import ray\n",
    "from ray import serve\n",
    "from fastapi import FastAPI\n",
    "import requests \n",
    "\n",
    "# library for pre-trained models\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb04476-1b36-4855-867d-b6ff4e11eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define a Ray Serve deployment\n",
    "# This decorator registers the class as a Ray Serve deployment\n",
    "@serve.deployment(num_replicas=2) # num_replicas specifies the number of replicas for load balancing\n",
    "@serve.ingress(app) # This decorator allows the FastAPI app to be served by Ray Serve\n",
    "class MySentimentModel:\n",
    "    def __init__(self):\n",
    "        # Load a pre-trained sentiment analysis model\n",
    "        self.model = pipeline(\"sentiment-analysis\",\n",
    "                              model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "    # Define any necessary application logic or transformation logic\n",
    "    def application_logic(self, text):\n",
    "        \"\"\"        Apply any necessary application logic to the input text.\n",
    "        \"\"\"\n",
    "        # simple application logic: truncate text if it exceeds a certain length\n",
    "        if len(text) > 50:\n",
    "            return text[:50].lower()  # Truncate and convert to lowercase\n",
    "        else:\n",
    "            return text.lower()\n",
    "        \n",
    "    @app.get(\"/predict\") # Define an endpoint for predictions\n",
    "    def predict(self, text: str):\n",
    "        \"\"\"        Predict sentiment for the given text.\n",
    "        \"\"\"\n",
    "        # Define any necessary application logic or transformation logic\n",
    "        text = self.application_logic(text) # Apply any necessary application logic to the input text\n",
    "\n",
    "        # Use the model to make a prediction\n",
    "        result = self.model(text)\n",
    "        return {\"text\": text, \"sentiment\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be12bd81-73e1-4c66-a5c7-777b1f3fe401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 16:33:04,056\tINFO worker.py:1918 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(ProxyActor pid=60476)\u001b[0m INFO 2025-08-02 16:33:06,865 proxy 127.0.0.1 -- Proxy starting on node 8a746934481c99344a7fa35fccaa9d0823d0a71116ac4b043ca84bb2 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=60476)\u001b[0m INFO 2025-08-02 16:33:06,913 proxy 127.0.0.1 -- Got updated endpoints: {}.\n",
      "INFO 2025-08-02 16:33:07,204 serve 57504 -- Started Serve in namespace \"serve\".\n",
      "\u001b[36m(ServeController pid=60472)\u001b[0m INFO 2025-08-02 16:33:10,018 controller 60472 -- Deploying new version of Deployment(name='MySentimentModel', app='default') (initial target replicas: 2).\n",
      "\u001b[36m(ProxyActor pid=60476)\u001b[0m INFO 2025-08-02 16:33:10,026 proxy 127.0.0.1 -- Got updated endpoints: {Deployment(name='MySentimentModel', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=60476)\u001b[0m INFO 2025-08-02 16:33:10,036 proxy 127.0.0.1 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x12d87b920>.\n",
      "\u001b[36m(ServeController pid=60472)\u001b[0m INFO 2025-08-02 16:33:10,125 controller 60472 -- Adding 2 replicas to Deployment(name='MySentimentModel', app='default').\n",
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=60479)\u001b[0m Device set to use mps:0\n",
      "INFO 2025-08-02 16:33:23,134 serve 57504 -- Application 'default' is ready at http://127.0.0.1:8000/.\n",
      "INFO 2025-08-02 16:33:23,138 serve 57504 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x17386dac0>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeploymentHandle(deployment='MySentimentModel')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=60479)\u001b[0m INFO 2025-08-02 16:35:07,451 default_MySentimentModel xfbcpg1v ad44d678-d468-474e-934c-3fc989df3af5 -- GET /predict 200 1131.0ms\n",
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=60479)\u001b[0m INFO 2025-08-02 16:37:00,868 default_MySentimentModel xfbcpg1v 4d8caf0e-f709-46fc-b7b3-d0c4e85b1d10 -- GET /predict 200 61.2ms\n",
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=60478)\u001b[0m INFO 2025-08-02 16:37:37,338 default_MySentimentModel yb74mu73 a3df9ceb-3ebe-464c-b92a-e6ea260360fb -- GET /predict 200 1238.7ms\n",
      "\u001b[36m(ServeReplica:default:MySentimentModel pid=60478)\u001b[0m INFO 2025-08-02 16:38:06,523 default_MySentimentModel yb74mu73 ad5e9c52-96d3-4e39-83ac-9a2378cbb711 -- GET /predict 200 568.3ms\n"
     ]
    }
   ],
   "source": [
    "serve.run(MySentimentModel.bind())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92bb41f5-930f-4c66-94b7-b1199ff169cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(text_payload :str):\n",
    "    response = requests.get(\"http://localhost:8000/predict\", params={\"text\": text_payload})\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24cb582a-292f-4b8a-b403-ee07ac05cf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'there are libraries built on top of ray',\n",
       " 'sentiment': [{'label': 'POSITIVE', 'score': 0.8838603496551514}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"There are libraries built on top of Ray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e89ea4e-21a4-4489-8a2a-71055c8912f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'edinburgh has a buzzing ml community',\n",
       " 'sentiment': [{'label': 'POSITIVE', 'score': 0.9889107942581177}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"Edinburgh has a buzzing ML community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be91780b-2ce9-4ab9-8db3-49f7d722daa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'too much heat drains me',\n",
       " 'sentiment': [{'label': 'NEGATIVE', 'score': 0.999446451663971}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"Too much heat drains me\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc27ab4-a150-4941-9b0a-81f03286f819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
