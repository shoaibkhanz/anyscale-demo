{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9a6cc8-72b9-44a4-96cf-07eb15612541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import ray serve and FastAPI libraries\n",
    "import ray\n",
    "from ray import serve\n",
    "from fastapi import FastAPI\n",
    "import requests \n",
    "\n",
    "# library for pre-trained models\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cb04476-1b36-4855-867d-b6ff4e11eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define a Ray Serve deployment\n",
    "# This decorator registers the class as a Ray Serve deployment\n",
    "@serve.deployment(num_replicas=2) # num_replicas specifies the number of replicas for load balancing\n",
    "@serve.ingress(app) # This decorator allows the FastAPI app to be served by Ray Serve\n",
    "class MySentimentModel:\n",
    "    def __init__(self):\n",
    "        # Load a pre-trained sentiment analysis model\n",
    "        self.model = pipeline(\"sentiment-analysis\",\n",
    "                              model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "    # Define any necessary application logic or transformation logic\n",
    "    def application_logic(self, text):\n",
    "        \"\"\"        Apply any necessary application logic to the input text.\n",
    "        \"\"\"\n",
    "        # simple application logic: truncate text if it exceeds a certain length\n",
    "        if len(text) > 50:\n",
    "            return text[:50].lower()  # Truncate and convert to lowercase\n",
    "        else:\n",
    "            return text.lower()\n",
    "        \n",
    "    @app.get(\"/predict\") # Define an endpoint for predictions\n",
    "    def predict(self, text: str):\n",
    "        \"\"\"        Predict sentiment for the given text.\n",
    "        \"\"\"\n",
    "        # Define any necessary application logic or transformation logic\n",
    "        text = self.application_logic(text) # Apply any necessary application logic to the input text\n",
    "\n",
    "        # Use the model to make a prediction\n",
    "        result = self.model(text)\n",
    "        return {\"text\": text, \"sentiment\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be12bd81-73e1-4c66-a5c7-777b1f3fe401",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 15:48:38,829\tINFO worker.py:1747 -- Connecting to existing Ray cluster at address: 100.77.61.72:6379...\n",
      "2025-08-02 15:48:38,840\tINFO worker.py:1918 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-xsclvf1y3h8ri22vxrxzy7b516.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2025-08-02 15:48:38,842\tINFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_fefd6d66be450f94ac647d52611cb36898e3dd4f.zip' (0.13MiB) to Ray cluster...\n",
      "2025-08-02 15:48:38,843\tINFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_fefd6d66be450f94ac647d52611cb36898e3dd4f.zip'.\n",
      "\u001b[36m(ProxyActor pid=6804)\u001b[0m INFO 2025-08-02 15:48:45,966 proxy 100.77.61.72 -- Proxy starting on node 06627b0ac8fc4ef17224267e74272b5b4c65596a323b8e70e242b4d3 (HTTP port: 8000).\n",
      "\u001b[36m(ProxyActor pid=6804)\u001b[0m INFO 2025-08-02 15:48:46,020 proxy 100.77.61.72 -- Got updated endpoints: {}.\n",
      "INFO 2025-08-02 15:48:46,050 serve 6518 -- Started Serve in namespace \"serve\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +28s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(ServeController pid=6740)\u001b[0m INFO 2025-08-02 15:48:50,269 controller 6740 -- Deploying new version of Deployment(name='MySentimentModel', app='default') (initial target replicas: 2).\n",
      "\u001b[36m(ProxyActor pid=6804)\u001b[0m INFO 2025-08-02 15:48:50,281 proxy 100.77.61.72 -- Got updated endpoints: {Deployment(name='MySentimentModel', app='default'): EndpointInfo(route='/', app_is_cross_language=False)}.\n",
      "\u001b[36m(ProxyActor pid=6804)\u001b[0m INFO 2025-08-02 15:48:50,313 proxy 100.77.61.72 -- Started <ray.serve._private.router.SharedRouterLongPollClient object at 0x7b2c1a9cd1c0>.\n",
      "\u001b[36m(ServeController pid=6740)\u001b[0m INFO 2025-08-02 15:48:50,381 controller 6740 -- Adding 2 replicas to Deployment(name='MySentimentModel', app='default').\n"
     ]
    }
   ],
   "source": [
    "serve.run(MySentimentModel.bind())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92bb41f5-930f-4c66-94b7-b1199ff169cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(text_payload :str):\n",
    "    response = requests.get(\"http://localhost:8000/predict\", params={\"text\": text_payload})\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24cb582a-292f-4b8a-b403-ee07ac05cf6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'there are libraries built on top of ray',\n",
       " 'sentiment': [{'label': 'POSITIVE', 'score': 0.8838600516319275}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"There are libraries built on top of Ray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e89ea4e-21a4-4489-8a2a-71055c8912f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'edinburgh has a buzzing ml community',\n",
       " 'sentiment': [{'label': 'POSITIVE', 'score': 0.9889107942581177}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"Edinburgh has a buzzing ML community\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be91780b-2ce9-4ab9-8db3-49f7d722daa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'too much heat drains me',\n",
       " 'sentiment': [{'label': 'NEGATIVE', 'score': 0.999446451663971}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(\"Too much heat drains me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd15fd4",
   "metadata": {},
   "source": [
    "## Build Anyscale service now\n",
    "- Create ray serve deployment script\n",
    "- Create an Image (if needed) using anyscale container images \n",
    "- Populate the service.yaml file\n",
    "- deploy the service `anyscale service deploy -f service.yaml`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d104a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_sentiment(\n",
    "    text: str,\n",
    "    base_url: str = \"https://sentiment-service-hq3g3.cld-bgtd6qup2pckeg3i.s.anyscaleuserdata.com\",\n",
    "    token: str = \"LSbnFASf6jf3zP3gkyuPmc78TbmjPA34S7hsjF52UFE\",\n",
    "    route: str = \"/predict\",\n",
    "    timeout: float = 10.0,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Query the deployed Ray Serve sentiment model.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text to analyze.\n",
    "        base_url (str): The base URL of the service endpoint.\n",
    "        token (str): The Bearer token for authentication.\n",
    "        route (str): The route of the prediction endpoint.\n",
    "        timeout (float): Request timeout in seconds.\n",
    "\n",
    "    Returns:\n",
    "        dict: The JSON response from the service.\n",
    "    \"\"\"\n",
    "    url = f\"{base_url}{route}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    params = {\"text\": text}\n",
    "    resp = requests.get(url, headers=headers, params=params)\n",
    "    return resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8772058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'there are libraries built on top of ray', 'sentiment': [{'label': 'POSITIVE', 'score': 0.8838600516319275}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = query_sentiment(\"There are libraries built on top of Ray\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bc27ab4-a150-4941-9b0a-81f03286f819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'edinburgh has a buzzing ml community', 'sentiment': [{'label': 'POSITIVE', 'score': 0.9889109134674072}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = query_sentiment(\"Edinburgh has a buzzing ML community\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "024cd89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'too much heat drains me', 'sentiment': [{'label': 'NEGATIVE', 'score': 0.999446451663971}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = query_sentiment(\"Too much heat drains me\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
