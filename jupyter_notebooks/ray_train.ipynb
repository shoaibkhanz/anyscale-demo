{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e9b6c7b4-cccb-4f1d-86bd-d2b0ec60bde6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.train import ScalingConfig,RunConfig,CheckpointConfig\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import resnet18 , ResNet18_Weights\n",
    "from torchvision.models import VisionTransformer\n",
    "from torchvision.transforms import ToTensor, Compose,Normalize\n",
    "from torch.utils.data import Subset,DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.classification import Accuracy\n",
    "from PIL import Image\n",
    "from filelock import FileLock\n",
    "from pathlib import Path\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5352278e-e560-4145-9561-a479807ac37c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ../marimo_notebooks/data\n",
       "    Split: Test"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CIFAR10(root=\"../marimo_notebooks/data\",download=True,train=False)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925827ef-e757-4c76-8926-1c873cb48faf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=32x32>, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data contains of a PIL image and the label\n",
    "next(iter(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9205da88-0ee7-45c8-8835-7c3d196f2c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'airplane': 0,\n",
       " 'automobile': 1,\n",
       " 'bird': 2,\n",
       " 'cat': 3,\n",
       " 'deer': 4,\n",
       " 'dog': 5,\n",
       " 'frog': 6,\n",
       " 'horse': 7,\n",
       " 'ship': 8,\n",
       " 'truck': 9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_to_idx = data.class_to_idx\n",
    "class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be82cd1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "46641dcc-4f1b-409c-b488-53966744e3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = resnet18(weights=ResNet18_Weights)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b0b185cc-5db5-4d35-90d3-bcd9769e29c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cifar_dataloader(batch_size):\n",
    "    #imagenet_transforms = ResNet18_Weights.IMAGENET1K_V1.transforms\n",
    "    #full_transform = imagenet_transforms()\n",
    "    full_transform = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    with FileLock(os.path.expanduser(\"~/cifar_data.lock\")):\n",
    "        train = CIFAR10(\n",
    "            root=\"~/cifar_data\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=full_transform,\n",
    "        )\n",
    "        valid = CIFAR10(\n",
    "            root=\"~/cifar_data\",\n",
    "            train=False,\n",
    "            download=True,\n",
    "            transform=full_transform,\n",
    "        )\n",
    "    train_sub = Subset(train,indices=range(1000))\n",
    "    valid_sub = Subset(valid,indices=range(1000))\n",
    "    # dataloaders to get data in batches\n",
    "    train_dataloader = DataLoader(train_sub, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_sub, batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader, valid_dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8ebd4958",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataloader = next(iter(get_cifar_dataloader(3)))\n",
    "single_batch =  next(iter(sample_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fbb0009f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eae4d223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 32, 32])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2919c541-40c1-41cd-af91-7e2610bbb33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "\n",
    "    epochs = config[\"epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    lr = config[\"lr\"]\n",
    "    \n",
    "    \n",
    "    # use detected device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "    # metrics\n",
    "    accuracy = Accuracy(task=\"multiclass\", num_classes=config[\"num_classes\"]).to(device)\n",
    "    \n",
    "    device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "    # model = resnet18(weights=weights)\n",
    "    # model = VisionTransformer(\n",
    "    #     image_size=32,   # CIFAR-10 image size is 32x32\n",
    "    #     patch_size=4,    # Patch size is 4x4\n",
    "    #     num_layers=3,   # Number of transformer layers\n",
    "    #     num_heads=2,     # Number of attention heads\n",
    "    #     hidden_dim=128,  # Hidden size (can be adjusted)\n",
    "    #     mlp_dim=64,     # MLP dimension (can be adjusted)\n",
    "    #     num_classes=10   # CIFAR-10 has 10 classes\n",
    "    # )\n",
    "    model = VisionTransformer(\n",
    "        image_size=32,   # CIFAR-10 image size is 32x32\n",
    "        patch_size=4,    # Patch size is 4x4\n",
    "        num_layers=12,   # Number of transformer layers\n",
    "        num_heads=8,     # Number of attention heads\n",
    "        hidden_dim=384,  # Hidden size (can be adjusted)\n",
    "        mlp_dim=768,     # MLP dimension (can be adjusted)\n",
    "        num_classes=10   # CIFAR-10 has 10 classes\n",
    "    )\n",
    "    #model = SimpleModel(in_channels=3,hidden_features=128,out_features=10)\n",
    "    # for parameter in model.parameters():\n",
    "    #     parameter.requires_grad = False\n",
    "    # model.fc = nn.Linear(512,config[\"num_classes\"],bias=True)\n",
    "    \n",
    "    model = ray.train.torch.prepare_model(model)\n",
    " \n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "\n",
    "    train_dataloader, valid_dataloader = get_cifar_dataloader(batch_size=batch_size)\n",
    "    train_dataloader = ray.train.torch.prepare_data_loader(train_dataloader)\n",
    "    valid_dataloader = ray.train.torch.prepare_data_loader(valid_dataloader)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # checking if training is scheduled in a distributed setting or not.\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_dataloader.sampler.set_epoch(epoch)\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        num_total = 0.0\n",
    "        num_correct = 0.0\n",
    "        #num_batches = 0.0\n",
    "        model.train()\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            x, y = batch[0], batch[1]\n",
    "            y_preds = model(x)\n",
    "            y_labels = y_preds.argmax(dim=1)\n",
    "            loss = loss_fn(y_preds,y)\n",
    "            acc = accuracy(y_labels,y)\n",
    "            train_loss +=  loss.item()\n",
    "            train_acc += acc.item()\n",
    "            num_total += y.shape[0]\n",
    "            num_correct += (y_labels == y).sum().item()\n",
    "        train_loss /=len(train_dataloader)\n",
    "        train_acc /=len(train_dataloader)\n",
    "        check_acc = num_correct / num_total\n",
    "        metrics = {\"epoch\":epoch,\"train_loss\":train_loss, \"train_acc\":train_acc,\"check_acc\": check_acc}\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.module.state_dict(),\n",
    "                os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics,\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            print(metrics)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eead0973-1ef0-4c91-b313-f986c12cc0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_batch_size = 300\n",
    "num_workers = 4\n",
    "use_gpu = True\n",
    "\n",
    "train_config = {\n",
    "    \"lr\": 0.01,\n",
    "    \"epochs\": 50,\n",
    "    \"num_classes\": 10,\n",
    "    \"batch_size\": global_batch_size // num_workers,\n",
    "    \"weight_decay\": 0.02\n",
    "}\n",
    "scaling_config = ScalingConfig(num_workers=num_workers, use_gpu=use_gpu)\n",
    "run_config = RunConfig(\n",
    "    #storage_path=str(Path(\"../marimo_notebooks/data/storage_path\").resolve()), \n",
    "    storage_path=\"/mnt/cluster_storage\",\n",
    "    name=f\"ray_train_torch_run-{uuid.uuid4().hex}\",\n",
    "    checkpoint_config = CheckpointConfig(num_to_keep=1,\n",
    "    checkpoint_score_attribute=\"train_acc\",\n",
    "    checkpoint_score_order=\"max\",) \n",
    ")\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_func,\n",
    "    train_loop_config=train_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "77710997-5f7f-479f-a4da-1f4549d71036",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:50:06,334\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:06 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/provider:aws)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:11 (running for 00:00:05.13)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/provider:aws)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "\u001b[36m(autoscaler +1h33m1s)\u001b[0m [autoscaler] [1xT4:8CPU-32GB] Attempting to add 4 nodes to the cluster (increasing from 0 to 4).\n",
      "\u001b[36m(autoscaler +1h33m1s)\u001b[0m [autoscaler] [1xT4:8CPU-32GB|g4dn.2xlarge] [us-east-2a] [on-demand] Launched 4 instances.\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:16 (running for 00:00:10.15)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:21 (running for 00:00:15.17)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:26 (running for 00:00:20.20)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/provider:aws, 0.0/1.0 anyscale/region:us-east-2)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:31 (running for 00:00:25.22)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/provider:aws, 0.0/1.0 anyscale/region:us-east-2)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:36 (running for 00:00:30.24)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/provider:aws, 0.0/1.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:41 (running for 00:00:35.26)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/provider:aws, 0.0/1.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:46 (running for 00:00:40.28)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/provider:aws, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:51 (running for 00:00:45.30)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/0 CPUs, 0/0 GPUs (0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/provider:aws, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:50:56 (running for 00:00:50.32)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/16 CPUs, 0/2 GPUs (0.0/3.0 anyscale/provider:aws, 0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/3.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "\u001b[36m(autoscaler +1h33m46s)\u001b[0m [autoscaler] Cluster upscaled to {16 CPU, 2 GPU}.\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:01 (running for 00:00:55.34)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/16 CPUs, 0/2 GPUs (0.0/3.0 anyscale/provider:aws, 0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/3.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:51:06,612\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 4.0 GPUs, but the cluster only has 16.0 CPUs and 2.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:06 (running for 00:01:00.37)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/16 CPUs, 0/2 GPUs (0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/3.0 anyscale/region:us-east-2, 0.0/3.0 anyscale/provider:aws, 0.0/2.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/2.0 accelerator_type:T4, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:11 (running for 00:01:05.39)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/16 CPUs, 0/2 GPUs (0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/3.0 anyscale/region:us-east-2, 0.0/3.0 anyscale/provider:aws, 0.0/2.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/2.0 accelerator_type:T4, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:16 (running for 00:01:10.41)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/5.0 anyscale/provider:aws, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/4.0 accelerator_type:T4, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "\u001b[36m(autoscaler +1h34m6s)\u001b[0m [autoscaler] Cluster upscaled to {32 CPU, 4 GPU}.\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:21 (running for 00:01:15.43)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/5.0 anyscale/provider:aws, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/4.0 accelerator_type:T4, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:26 (running for 00:01:20.45)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:31 (running for 00:01:25.47)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:36 (running for 00:01:30.50)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:41 (running for 00:01:35.52)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:46 (running for 00:01:40.54)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/5.0 anyscale/region:us-east-2, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:51 (running for 00:01:45.56)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/5.0 anyscale/region:us-east-2, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:51:56 (running for 00:01:50.58)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:01 (running for 00:01:55.60)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:52:06,671\tWARNING insufficient_resources_manager.py:163 -- Ignore this message if the cluster is autoscaling. Training has not started in the last 60 seconds. This could be due to the cluster not having enough resources available. You asked for 1.0 CPUs and 4.0 GPUs, but the cluster only has 16.0 CPUs and 2.0 GPUs available. Stop the training and adjust the required resources (e.g. via the `ScalingConfig` or `resources_per_trial`, or `num_workers` for rllib), or add more resources to your cluster.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:06 (running for 00:02:00.63)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/region:us-east-2, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:11 (running for 00:02:05.65)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/region:us-east-2, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:17 (running for 00:02:10.67)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:22 (running for 00:02:15.69)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:27 (running for 00:02:20.72)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/5.0 anyscale/provider:aws, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:32 (running for 00:02:25.74)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 0/32 CPUs, 0/4 GPUs (0.0/5.0 anyscale/provider:aws, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:37 (running for 00:02:30.79)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:42 (running for 00:02:35.84)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m Setting up process group for: env:// [rank=0, world_size=4]\n",
      "\u001b[36m(TorchTrainer pid=2955, ip=100.86.214.127)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=2955, ip=100.86.214.127)\u001b[0m - (node_id=167ec187d72cd6790239e350d5242986d9423df372cf8e81a1fb1e2d, ip=100.86.214.127, pid=3036) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=2955, ip=100.86.214.127)\u001b[0m - (node_id=0801d83110f8c752cf99adf9e60508702179960a8c89c679bd3e0c22, ip=100.103.56.113, pid=2924) world_rank=1, local_rank=0, node_rank=1\n",
      "\u001b[36m(TorchTrainer pid=2955, ip=100.86.214.127)\u001b[0m - (node_id=ce9e615b8df1abbe9ae8a87dac30e3f3e594896615f15a80e8e2513e, ip=100.95.108.125, pid=2980) world_rank=2, local_rank=0, node_rank=2\n",
      "\u001b[36m(TorchTrainer pid=2955, ip=100.86.214.127)\u001b[0m - (node_id=b73bae5408d674329b4850702b54c452279597f1bc867f3e02780564, ip=100.102.153.87, pid=3037) world_rank=3, local_rank=0, node_rank=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:47 (running for 00:02:40.86)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=2924, ip=100.103.56.113)\u001b[0m Wrapping provided model in DistributedDataParallel.\n",
      "  0%|          | 0.00/170M [00:00<?, ?B/s]56.113)\u001b[0m \n",
      "  0%|          | 328k/170M [00:00<00:57, 2.99MB/s][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:52 (running for 00:02:45.88)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/region:us-east-2, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3037, ip=100.102.153.87)\u001b[0m Moving model to device: cuda:0\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=2980, ip=100.95.108.125)\u001b[0m Wrapping provided model in DistributedDataParallel.\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "  0%|          | 0.00/170M [00:00<?, ?B/s]\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      " 52%|█████▏    | 89.4M/170M [00:05<00:04, 16.3MB/s]\u001b[32m [repeated 197x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:52:57 (running for 00:02:50.90)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 157M/170M [00:08<00:00, 18.3MB/s][0m \n",
      " 92%|█████████▏| 157M/170M [00:08<00:00, 18.4MB/s][0m \n",
      " 92%|█████████▏| 157M/170M [00:08<00:00, 18.5MB/s][0m \n",
      " 92%|█████████▏| 157M/170M [00:08<00:00, 18.5MB/s][0m \n",
      " 93%|█████████▎| 159M/170M [00:08<00:00, 18.6MB/s][0m \n",
      " 93%|█████████▎| 159M/170M [00:08<00:00, 18.7MB/s][0m \n",
      " 93%|█████████▎| 159M/170M [00:08<00:00, 18.7MB/s][0m \n",
      " 93%|█████████▎| 159M/170M [00:08<00:00, 18.7MB/s][0m \n",
      " 94%|█████████▍| 161M/170M [00:08<00:00, 18.7MB/s][0m \n",
      " 94%|█████████▍| 161M/170M [00:08<00:00, 18.7MB/s][0m \n",
      " 94%|█████████▍| 161M/170M [00:08<00:00, 18.7MB/s][0m \n",
      " 94%|█████████▍| 161M/170M [00:08<00:00, 18.8MB/s][0m \n",
      " 96%|█████████▌| 163M/170M [00:08<00:00, 18.8MB/s][0m \n",
      " 95%|█████████▌| 163M/170M [00:09<00:00, 18.8MB/s][0m \n",
      " 96%|█████████▌| 163M/170M [00:09<00:00, 18.9MB/s][0m \n",
      " 95%|█████████▌| 163M/170M [00:09<00:00, 18.9MB/s][0m \n",
      " 97%|█████████▋| 165M/170M [00:09<00:00, 18.9MB/s][0m \n",
      " 97%|█████████▋| 165M/170M [00:09<00:00, 19.0MB/s][0m \n",
      " 97%|█████████▋| 165M/170M [00:09<00:00, 19.0MB/s][0m \n",
      " 97%|█████████▋| 165M/170M [00:09<00:00, 19.0MB/s][0m \n",
      " 98%|█████████▊| 167M/170M [00:09<00:00, 19.1MB/s][0m \n",
      " 98%|█████████▊| 167M/170M [00:09<00:00, 19.1MB/s][0m \n",
      " 98%|█████████▊| 167M/170M [00:09<00:00, 19.2MB/s][0m \n",
      " 98%|█████████▊| 167M/170M [00:09<00:00, 19.2MB/s][0m \n",
      "100%|██████████| 170M/170M [00:09<00:00, 18.2MB/s][0m \n",
      " 99%|█████████▉| 169M/170M [00:09<00:00, 19.2MB/s][0m \n",
      " 99%|█████████▉| 169M/170M [00:09<00:00, 19.2MB/s][0m \n",
      " 99%|█████████▉| 169M/170M [00:09<00:00, 19.2MB/s][0m \n",
      "100%|██████████| 170M/170M [00:09<00:00, 18.1MB/s][0m \n",
      "100%|██████████| 170M/170M [00:09<00:00, 18.2MB/s][0m \n",
      "100%|██████████| 170M/170M [00:09<00:00, 18.1MB/s][0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:02 (running for 00:02:55.91)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2980, ip=100.95.108.125)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000000)\n",
      " 91%|█████████ | 155M/170M [00:08<00:00, 18.2MB/s]\u001b[32m [repeated 138x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 0, 'train_loss': 2.302584648132324, 'train_acc': 0.08000000193715096, 'check_acc': 0.088}\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 1, 'train_loss': 2.302584648132324, 'train_acc': 0.08666666597127914, 'check_acc': 0.096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:04,230\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:05,008\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 2, 'train_loss': 2.302584648132324, 'train_acc': 0.08000000193715096, 'check_acc': 0.088}\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 3, 'train_loss': 2.302584648132324, 'train_acc': 0.10333333350718021, 'check_acc': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:06,241\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:06,860\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 4, 'train_loss': 2.302584648132324, 'train_acc': 0.0800000000745058, 'check_acc': 0.08}\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:07 (running for 00:03:00.92)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:07,873\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 5, 'train_loss': 2.302584648132324, 'train_acc': 0.09666666574776173, 'check_acc': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3037, ip=100.102.153.87)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000006)\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "2025-08-02 14:53:08,765\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 6, 'train_loss': 2.302584648132324, 'train_acc': 0.09999999776482582, 'check_acc': 0.112}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:09,799\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 7, 'train_loss': 2.302584648132324, 'train_acc': 0.08333333395421505, 'check_acc': 0.092}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:10,695\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 8, 'train_loss': 2.302584648132324, 'train_acc': 0.08999999985098839, 'check_acc': 0.1}\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 9, 'train_loss': 2.302584648132324, 'train_acc': 0.10333333536982536, 'check_acc': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:11,780\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:12,441\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:12 (running for 00:03:05.93)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 10, 'train_loss': 2.302584648132324, 'train_acc': 0.09999999776482582, 'check_acc': 0.096}\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 11, 'train_loss': 2.302584648132324, 'train_acc': 0.1133333332836628, 'check_acc': 0.112}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:13,718\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000012)\u001b[32m [repeated 24x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 12, 'train_loss': 2.302584648132324, 'train_acc': 0.07999999821186066, 'check_acc': 0.088}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:14,621\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 13, 'train_loss': 2.302584648132324, 'train_acc': 0.10666666738688946, 'check_acc': 0.112}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:15,605\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:16,541\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 14, 'train_loss': 2.302584648132324, 'train_acc': 0.07666666805744171, 'check_acc': 0.084}\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 15, 'train_loss': 2.302584648132324, 'train_acc': 0.08999999985098839, 'check_acc': 0.084}\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:17 (running for 00:03:11.00)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 accelerator_type:T4, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:17,548\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 16, 'train_loss': 2.302584648132324, 'train_acc': 0.13333333283662796, 'check_acc': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:18,502\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:19,224\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 17, 'train_loss': 2.302584648132324, 'train_acc': 0.09333333373069763, 'check_acc': 0.096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2924, ip=100.103.56.113)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000018)\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "2025-08-02 14:53:20,302\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 18, 'train_loss': 2.302584648132324, 'train_acc': 0.1133333332836628, 'check_acc': 0.112}\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 19, 'train_loss': 2.302584648132324, 'train_acc': 0.086666664108634, 'check_acc': 0.096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:21,443\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 20, 'train_loss': 2.302584648132324, 'train_acc': 0.11999999731779099, 'check_acc': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:22,464\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:22 (running for 00:03:16.12)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 accelerator_type:T4, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 21, 'train_loss': 2.302584648132324, 'train_acc': 0.12333333119750023, 'check_acc': 0.116}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:23,824\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 22, 'train_loss': 2.302584648132324, 'train_acc': 0.10666666552424431, 'check_acc': 0.104}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:24,648\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 23, 'train_loss': 2.302584648132324, 'train_acc': 0.1333333309739828, 'check_acc': 0.112}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2924, ip=100.103.56.113)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000023)\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
      "2025-08-02 14:53:25,515\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 24, 'train_loss': 2.302584648132324, 'train_acc': 0.12666666507720947, 'check_acc': 0.128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:26,411\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 25, 'train_loss': 2.302584648132324, 'train_acc': 0.07333333417773247, 'check_acc': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:27,294\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:27 (running for 00:03:21.15)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:28,201\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 26, 'train_loss': 2.302584648132324, 'train_acc': 0.10000000149011612, 'check_acc': 0.096}\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 27, 'train_loss': 2.302584648132324, 'train_acc': 0.13333333283662796, 'check_acc': 0.128}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:29,293\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:29,888\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 28, 'train_loss': 2.302584648132324, 'train_acc': 0.06333333440124989, 'check_acc': 0.068}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2980, ip=100.95.108.125)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000029)\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
      "2025-08-02 14:53:30,905\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 29, 'train_loss': 2.302584648132324, 'train_acc': 0.09333333186805248, 'check_acc': 0.096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:31,800\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 30, 'train_loss': 2.302584648132324, 'train_acc': 0.12666666507720947, 'check_acc': 0.12}\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:32 (running for 00:03:26.16)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/provider:aws, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 31, 'train_loss': 2.302584648132324, 'train_acc': 0.11999999731779099, 'check_acc': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:32,740\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:33,622\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 32, 'train_loss': 2.302584648132324, 'train_acc': 0.12333333119750023, 'check_acc': 0.124}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:34,644\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 33, 'train_loss': 2.302584648132324, 'train_acc': 0.07666666619479656, 'check_acc': 0.076}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:35,532\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 34, 'train_loss': 2.302584648132324, 'train_acc': 0.07666666619479656, 'check_acc': 0.076}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=2924, ip=100.103.56.113)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000035)\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "2025-08-02 14:53:36,517\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 35, 'train_loss': 2.302584648132324, 'train_acc': 0.08666666597127914, 'check_acc': 0.096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:37,422\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 36, 'train_loss': 2.302584648132324, 'train_acc': 0.10000000149011612, 'check_acc': 0.096}\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:37 (running for 00:03:31.18)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:38,463\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 37, 'train_loss': 2.302584648132324, 'train_acc': 0.1133333332836628, 'check_acc': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:39,361\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 38, 'train_loss': 2.302584648132324, 'train_acc': 0.09666666761040688, 'check_acc': 0.092}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:40,340\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 39, 'train_loss': 2.302584648132324, 'train_acc': 0.09999999776482582, 'check_acc': 0.096}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:41,264\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 40, 'train_loss': 2.302584648132324, 'train_acc': 0.08333333395421505, 'check_acc': 0.092}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000041)\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "2025-08-02 14:53:42,193\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 41, 'train_loss': 2.302584648132324, 'train_acc': 0.12000000104308128, 'check_acc': 0.12}\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:42 (running for 00:03:36.25)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/5.0 anyscale/provider:aws, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/4.0 accelerator_type:T4, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/1.0 anyscale/cpu_only:true, 0.0/1.0 anyscale/node-group:head)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 42, 'train_loss': 2.302584648132324, 'train_acc': 0.08666666597127914, 'check_acc': 0.088}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:43,378\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:44,255\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 43, 'train_loss': 2.302584648132324, 'train_acc': 0.06666666641831398, 'check_acc': 0.072}\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 44, 'train_loss': 2.302584648132324, 'train_acc': 0.09333333745598793, 'check_acc': 0.104}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:45,203\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:46,020\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 45, 'train_loss': 2.302584648132324, 'train_acc': 0.12333333119750023, 'check_acc': 0.132}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:46,955\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 46, 'train_loss': 2.302584648132324, 'train_acc': 0.07000000029802322, 'check_acc': 0.076}\n",
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:47 (running for 00:03:41.31)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/5.0 anyscale/provider:aws, 0.0/4.0 accelerator_type:T4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 47, 'train_loss': 2.302584648132324, 'train_acc': 0.06333333346992731, 'check_acc': 0.068}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3037, ip=100.102.153.87)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000047)\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
      "2025-08-02 14:53:47,959\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n",
      "2025-08-02 14:53:48,833\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 48, 'train_loss': 2.302584648132324, 'train_acc': 0.08333333395421505, 'check_acc': 0.076}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:49,788\tWARNING experiment_state.py:206 -- Experiment state snapshotting has been triggered multiple times in the last 5.0 seconds and may become a bottleneck. A snapshot is forced if `CheckpointConfig(num_to_keep)` is set, and a trial has checkpointed >= `num_to_keep` times since the last snapshot.\n",
      "You may want to consider increasing the `CheckpointConfig(num_to_keep)` or decreasing the frequency of saving checkpoints.\n",
      "You can suppress this warning by setting the environment variable TUNE_WARN_EXCESSIVE_EXPERIMENT_CHECKPOINT_SYNC_THRESHOLD_S to a smaller value than the current threshold (5.0). Set it to 0 to completely suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=3036, ip=100.86.214.127)\u001b[0m {'epoch': 49, 'train_loss': 2.302584648132324, 'train_acc': 0.07333333324640989, 'check_acc': 0.072}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 14:53:51,377\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c' in 0.1863s.\n",
      "2025-08-02 14:53:51,379\tINFO tune.py:1041 -- Total run time: 225.05 seconds (224.84 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-08-02 14:53:51 (running for 00:03:45.03)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/32 CPUs, 4.0/4 GPUs (0.0/5.0 anyscale/provider:aws, 0.0/4.0 accelerator_type:T4, 0.0/4.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/5.0 anyscale/region:us-east-2, 0.0/4.0 anyscale/accelerator_shape:1xT4, 0.0/1.0 anyscale/node-group:head, 0.0/1.0 anyscale/cpu_only:true)\n",
      "Result logdir: /tmp/ray/session_2025-08-02_12-18-15_816680_2464/artifacts/2025-08-02_14-50-06/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n",
      "Training result: Result(\n",
      "  metrics={'epoch': 49, 'train_loss': 2.302584648132324, 'train_acc': 0.07333333324640989, 'check_acc': 0.072},\n",
      "  path='/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06',\n",
      "  filesystem='local',\n",
      "  checkpoint=Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray_train_torch_run-e68db446183641bd87f1ff86cb1a720c/TorchTrainer_faa35_00000_0_2025-08-02_14-50-06/checkpoint_000049)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()\n",
    "print(f\"Training result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ead92d3-a9d2-4591-b980-b645028aaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178d226b-52c0-493f-8fb3-ea67f60f2395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
